{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXJqEJbpCmYp",
        "outputId": "f98b0af6-8bb7-4d81-d0cb-e5882355e8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "hZJVUdACL4xA",
        "outputId": "8f933bad-320d-4151-f073-ba31c46754fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NVIDIA A100-SXM4-40GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FVqKEFp9J7XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qwen_vl_utils\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRXfz0KdKRvt",
        "outputId": "c7583846-215a-4a07-c4e1-fb7ac296be01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qwen_vl_utils\n",
            "  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting av (from qwen_vl_utils)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2025.1.31)\n",
            "Downloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\n",
            "Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av, qwen_vl_utils\n",
            "Successfully installed av-14.2.0 qwen_vl_utils-0.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nMBFUQYz4A05",
        "outputId": "4ff655c8-4b07-46f9-a248-62a6d1d20b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 345, done.\u001b[K\n",
            "remote: Counting objects: 100% (345/345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (280/280), done.\u001b[K\n",
            "remote: Total 345 (delta 85), reused 159 (delta 49), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (345/345), 9.60 MiB | 30.61 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.50.0)\n",
            "Collecting datasets<=3.4.1,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.0,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
            "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.2.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.14)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Collecting pydub (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26046 sha256=9d3a8f15a647e6412601760bb62c6e8d597a92bf29c7177dfe38fd664152ec74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v_39_k96/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=fa5f551598793335f8c5e821a5b02c6aac84d1ffc205ddf70657396ac301c958\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, markupsafe, groovy, fsspec, fire, ffmpy, dill, aiofiles, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, tokenizers, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, datasets, bitsandbytes, trl, llamafactory\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 bitsandbytes-0.45.4 datasets-3.4.1 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.2 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.1 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe",
                  "nvidia"
                ]
              },
              "id": "60173727c6614bacbbaede610e08c8e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FlSC8o4REv",
        "outputId": "c6b81058-3ca4-4bbe-8252-96d21a0ca4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.4.tar.gz (1.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.4-py3-none-any.whl size=1562649 sha256=a30fbe206685c5765d95385318384c48fd7bff996bf03280922418ce42a4ab85\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/6c/e5/ccad75c8ade9cb21e74721affd6d17820b1806249aac34f7f0\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.4 hjson-3.1.0 ninja-1.11.1.4\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187831595 sha256=58853b28a5a926cae14402bfd8d4d93a45ebf8f9e79533f37ab09d0d77a99c05\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.7.4.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_chinese"
      ],
      "metadata": {
        "id": "j36IWJg5pqxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4955ad9-8242-433a-fbc5-46e2d6a5bdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_chinese\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge_chinese) (1.17.0)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: rouge_chinese\n",
            "Successfully installed rouge_chinese-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyKbKR8_365v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZKoUKqEpq1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build training dataset"
      ],
      "metadata": {
        "id": "isFin-7FprVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_train.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "tuning_data = []\n",
        "\n",
        "for entry in data:\n",
        "  img_path = entry[\"image\"]\n",
        "  obj_type = entry[\"type\"]\n",
        "  obj_count = entry[\"solution\"]\n",
        "  problem = entry[\"problem\"]\n",
        "  conversation = {\n",
        "    \"conversations\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"<image>\\n{problem}. Please provide your answer as a number in the following format: count\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": str(obj_count)\n",
        "      }\n",
        "    ],\n",
        "    \"image_paths\": ['/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_train/'+img_path]\n",
        "  }\n",
        "  tuning_data.append(conversation)\n",
        "\n",
        "random.shuffle(tuning_data)\n",
        "\n",
        "print(tuning_data[0])\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/CrowdLLM/crowd_train.jsonl\"\n",
        "llama_path = \"/content/LLaMA-Factory/data/crowd_train.jsonl\"\n",
        "with open(llama_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  for item in tuning_data:\n",
        "    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mlKd_5qGPqL",
        "outputId": "4076dc64-0c90-4c8e-b0b0-8bff5599d9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conversations': [{'role': 'user', 'content': '<image>\\nPlease count the number of people in the image. Please provide your answer as a number in the following format: count'}, {'role': 'assistant', 'content': '78'}], 'image_paths': ['/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_train/GTA_Head/train/scene_22/img1/000045.jpg']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-JXnuVOJ1YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#build test dataset"
      ],
      "metadata": {
        "id": "IHxqhFSTp0rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_test.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "eval_data = []\n",
        "\n",
        "for entry in test_data:\n",
        "    img_path = entry[\"image\"]\n",
        "    obj_type = entry[\"type\"]\n",
        "    obj_count = entry[\"solution\"]  # This is your ground-truth count\n",
        "    problem = entry[\"problem\"]\n",
        "\n",
        "    eval_entry = {\n",
        "      \"conversations\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f\"<image>\\n{problem}. Please provide your answer as a number in the following format: count\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"assistant\",\n",
        "          \"content\": str(obj_count)  # Ground truth answer\n",
        "        }\n",
        "      ],\n",
        "      \"image_paths\": [\n",
        "          \"/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_test/\" + img_path\n",
        "      ]\n",
        "    }\n",
        "    eval_data.append(eval_entry)\n",
        "\n",
        "print(eval_data[0])\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/CrowdLLM/crowd_eval.jsonl\"\n",
        "llama_path = \"/content/LLaMA-Factory/data/crowd_eval.jsonl\"\n",
        "with open(llama_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in eval_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "8n1d5UWZGPsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95319f9-b1d9-4abe-fb52-80baf2ab0eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conversations': [{'role': 'user', 'content': '<image>\\nPlease count the number of sheep in the image. Please provide your answer as a number in the following format: count'}, {'role': 'assistant', 'content': '71'}], 'image_paths': ['/content/drive/MyDrive/CrowdLLM/new_datasets/vlm_r1_test/Aerial_Sheep.v1-v1.coco/test/DJI_0039_MOV-64_jpg.rf.4931d1329fc1401f10da1ba1f6021934.jpg']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCaeKi7frUhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add to LLaMA-Factory/data/dataset_info.json\n"
      ],
      "metadata": {
        "id": "EoyxQy4k-kx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # \"crowd_train\": {\n",
        "  #   \"file_name\": \"crowd_train.jsonl\",\n",
        "  #   \"formatting\": \"sharegpt\",\n",
        "  #   \"columns\": {\n",
        "  #     \"messages\": \"conversations\",\n",
        "  #     \"images\": \"image_paths\"\n",
        "  #   },\n",
        "  #   \"tags\": {\n",
        "  #     \"role_tag\": \"role\",\n",
        "  #     \"content_tag\": \"content\",\n",
        "  #     \"user_tag\": \"user\",\n",
        "  #     \"assistant_tag\": \"assistant\"\n",
        "  #   }\n",
        "  # },\n",
        "  # \"crowd_eval\": {\n",
        "  #   \"file_name\": \"crowd_eval.jsonl\",\n",
        "  #   \"formatting\": \"sharegpt\",\n",
        "  #   \"columns\": {\n",
        "  #     \"messages\": \"conversations\",\n",
        "  #     \"images\": \"image_paths\"\n",
        "  #   },\n",
        "  #   \"tags\": {\n",
        "  #     \"role_tag\": \"role\",\n",
        "  #     \"content_tag\": \"content\",\n",
        "  #     \"user_tag\": \"user\",\n",
        "  #     \"assistant_tag\": \"assistant\"\n",
        "  #   }\n",
        "  # },"
      ],
      "metadata": {
        "id": "0GfjQEEb-j9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8w_lKii_-nuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "yJkZFCodc5rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Either launch llama factory webui\n",
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "M7h0H4Qqy1G2",
        "outputId": "3c2f7e95-f66a-4097-9369-b621196fcee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-03-27 22:19:24.528837: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-27 22:19:24.547620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743113964.569912   23826 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743113964.576639   23826 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-27 22:19:24.599386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://0dd8efde3734d7af25.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2963, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 121, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 94, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2869, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2965, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\", line 152, in write_with_callbacks\n",
            "    callbacks_copy = list(callbacks.values())\n",
            "                          ^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://0dd8efde3734d7af25.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use command to do training/evaluation\n",
        "!llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen2_vl \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset crowd_train \\\n",
        "    --cutoff_len 2048 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --max_samples 100000 \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 100 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42 \\\n",
        "    --bf16 True \\\n",
        "    --plot_loss True \\\n",
        "    --trust_remote_code True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --optim adamw_torch \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --freeze_vision_tower False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D26Xf50kAl1r",
        "outputId": "a9238155-4422-492d-8e73-d6c525008522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-27 22:20:27.418024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-27 22:20:27.436577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743114027.458381   24156 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743114027.465010   24156 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-27 22:20:27.487540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-03-27 22:20:34] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,154 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-03-27 22:20:34,541 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|image_processing_base.py:381] 2025-03-27 22:20:34,709 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:381] 2025-03-27 22:20:34,753 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
            "[WARNING|logging.py:329] 2025-03-27 22:20:34,753 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO|image_processing_base.py:434] 2025-03-27 22:20:34,754 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-27 22:20:34,798 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-03-27 22:20:35,158 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|processing_utils.py:876] 2025-03-27 22:20:35,899 >> Processor Qwen2_5_VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
            "}\n",
            "\n",
            "[INFO|2025-03-27 22:20:35] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-03-27 22:20:35] llamafactory.data.loader:143 >> Loading dataset crowd_train.jsonl...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5501, 1760, 279, 1372, 315, 1251, 304, 279, 2168, 13, 5209, 3410, 697, 4226, 438, 264, 1372, 304, 279, 2701, 3561, 25, 1760, 151645, 198, 151644, 77091, 198, 22, 23, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>\n",
            "Please count the number of people in the image. Please provide your answer as a number in the following format: count<|im_end|>\n",
            "<|im_start|>assistant\n",
            "78<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 22, 23, 151645, 198]\n",
            "labels:\n",
            "78<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-27 22:20:36,347 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-27 22:20:36,349 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1154] 2025-03-27 22:20:36,363 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2170] 2025-03-27 22:20:36,364 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-03-27 22:20:36,366 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2170] 2025-03-27 22:20:36,367 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.36s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-03-27 22:20:39,182 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-03-27 22:20:39,183 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-03-27 22:20:39,230 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-03-27 22:20:39,231 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 1e-06\n",
            "}\n",
            "\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.model_utils.misc:143 >> Found linear modules: v_proj,qkv,q_proj,k_proj,gate_proj,up_proj,down_proj,proj,o_proj\n",
            "[INFO|2025-03-27 22:20:39] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.\n",
            "[INFO|2025-03-27 22:20:40] llamafactory.model.loader:143 >> trainable params: 20,542,464 || all params: 3,775,165,440 || trainable%: 0.5441\n",
            "[INFO|trainer.py:748] 2025-03-27 22:20:40,441 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:783] 2025-03-27 22:20:40,442 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|trainer.py:2409] 2025-03-27 22:20:40,950 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-03-27 22:20:40,950 >>   Num examples = 3,803\n",
            "[INFO|trainer.py:2411] 2025-03-27 22:20:40,950 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:2412] 2025-03-27 22:20:40,950 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-03-27 22:20:40,950 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2416] 2025-03-27 22:20:40,950 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2417] 2025-03-27 22:20:40,950 >>   Total optimization steps = 237\n",
            "[INFO|trainer.py:2418] 2025-03-27 22:20:40,959 >>   Number of trainable parameters = 20,542,464\n",
            "{'loss': 1.3337, 'grad_norm': 2.9018983840942383, 'learning_rate': 4.994510989166997e-05, 'epoch': 0.02, 'num_input_tokens_seen': 61376}\n",
            "{'loss': 1.216, 'grad_norm': 4.027134895324707, 'learning_rate': 4.978068060059929e-05, 'epoch': 0.04, 'num_input_tokens_seen': 122976}\n",
            "{'loss': 1.0689, 'grad_norm': 3.149676561355591, 'learning_rate': 4.950743417011591e-05, 'epoch': 0.06, 'num_input_tokens_seen': 181760}\n",
            "{'loss': 1.0606, 'grad_norm': 3.2596235275268555, 'learning_rate': 4.9126570482313424e-05, 'epoch': 0.08, 'num_input_tokens_seen': 242736}\n",
            "{'loss': 1.0787, 'grad_norm': 2.963770866394043, 'learning_rate': 4.863976198911845e-05, 'epoch': 0.11, 'num_input_tokens_seen': 304000}\n",
            "{'loss': 1.0159, 'grad_norm': 3.1024563312530518, 'learning_rate': 4.804914636820517e-05, 'epoch': 0.13, 'num_input_tokens_seen': 367344}\n",
            "{'loss': 1.0723, 'grad_norm': 5.663031101226807, 'learning_rate': 4.735731713600665e-05, 'epoch': 0.15, 'num_input_tokens_seen': 431360}\n",
            "{'loss': 0.9625, 'grad_norm': 16.082454681396484, 'learning_rate': 4.6567312259043e-05, 'epoch': 0.17, 'num_input_tokens_seen': 491104}\n",
            "{'loss': 0.8862, 'grad_norm': 11.850622177124023, 'learning_rate': 4.5682600813576435e-05, 'epoch': 0.19, 'num_input_tokens_seen': 552768}\n",
            "{'loss': 1.0505, 'grad_norm': 5.730424404144287, 'learning_rate': 4.470706775217355e-05, 'epoch': 0.21, 'num_input_tokens_seen': 615584}\n",
            "{'loss': 1.0292, 'grad_norm': 6.978216648101807, 'learning_rate': 4.3644996844067955e-05, 'epoch': 0.23, 'num_input_tokens_seen': 678144}\n",
            "{'loss': 0.9248, 'grad_norm': 5.679780960083008, 'learning_rate': 4.2501051864235636e-05, 'epoch': 0.25, 'num_input_tokens_seen': 738752}\n",
            "{'loss': 0.9521, 'grad_norm': 8.881064414978027, 'learning_rate': 4.128025611378594e-05, 'epoch': 0.27, 'num_input_tokens_seen': 800688}\n",
            "{'loss': 0.9618, 'grad_norm': 8.440853118896484, 'learning_rate': 3.998797036159812e-05, 'epoch': 0.29, 'num_input_tokens_seen': 863104}\n",
            "{'loss': 0.9562, 'grad_norm': 225.9862060546875, 'learning_rate': 3.862986930406669e-05, 'epoch': 0.32, 'num_input_tokens_seen': 929072}\n",
            "{'loss': 0.9312, 'grad_norm': 5.04384183883667, 'learning_rate': 3.721191664632531e-05, 'epoch': 0.34, 'num_input_tokens_seen': 991376}\n",
            "{'loss': 0.8947, 'grad_norm': 5.309420585632324, 'learning_rate': 3.57403389143732e-05, 'epoch': 0.36, 'num_input_tokens_seen': 1053280}\n",
            "{'loss': 0.8982, 'grad_norm': 4.038384437561035, 'learning_rate': 3.4221598113100195e-05, 'epoch': 0.38, 'num_input_tokens_seen': 1116608}\n",
            "{'loss': 0.9499, 'grad_norm': 9.964717864990234, 'learning_rate': 3.266236335027487e-05, 'epoch': 0.4, 'num_input_tokens_seen': 1179408}\n",
            "{'loss': 0.8107, 'grad_norm': 4.93572998046875, 'learning_rate': 3.1069481551100695e-05, 'epoch': 0.42, 'num_input_tokens_seen': 1240544}\n",
            " 42% 100/237 [52:37<1:07:03, 29.37s/it][INFO|trainer.py:3966] 2025-03-27 23:13:18,304 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100\n",
            "[INFO|configuration_utils.py:699] 2025-03-27 23:13:18,475 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-27 23:13:18,477 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-27 23:13:18,651 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-27 23:13:18,651 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:261] 2025-03-27 23:13:19,285 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-27 23:13:19,286 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-27 23:13:19,287 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/special_tokens_map.json\n",
            "[INFO|processing_utils.py:638] 2025-03-27 23:13:20,043 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-100/chat_template.json\n",
            "{'loss': 0.9547, 'grad_norm': 6.571334362030029, 'learning_rate': 2.9449947391938766e-05, 'epoch': 0.44, 'num_input_tokens_seen': 1302384}\n",
            "{'loss': 0.9463, 'grad_norm': 369.96441650390625, 'learning_rate': 2.7810872585224308e-05, 'epoch': 0.46, 'num_input_tokens_seen': 1363696}\n",
            "{'loss': 0.9219, 'grad_norm': 5.477662563323975, 'learning_rate': 2.6159454650453463e-05, 'epoch': 0.48, 'num_input_tokens_seen': 1426720}\n",
            "{'loss': 0.9294, 'grad_norm': 6.9683613777160645, 'learning_rate': 2.4502945308373246e-05, 'epoch': 0.5, 'num_input_tokens_seen': 1488208}\n",
            "{'loss': 0.8781, 'grad_norm': 32.03768539428711, 'learning_rate': 2.2848618637162584e-05, 'epoch': 0.53, 'num_input_tokens_seen': 1550480}\n",
            "{'loss': 0.8778, 'grad_norm': 9.84743595123291, 'learning_rate': 2.1203739130437146e-05, 'epoch': 0.55, 'num_input_tokens_seen': 1610960}\n",
            "{'loss': 0.9049, 'grad_norm': 11.2172269821167, 'learning_rate': 1.957552979734205e-05, 'epoch': 0.57, 'num_input_tokens_seen': 1672064}\n",
            "{'loss': 0.8931, 'grad_norm': 6.082320213317871, 'learning_rate': 1.7971140444811513e-05, 'epoch': 0.59, 'num_input_tokens_seen': 1731440}\n",
            "{'loss': 0.8764, 'grad_norm': 18.112197875976562, 'learning_rate': 1.639761628127465e-05, 'epoch': 0.61, 'num_input_tokens_seen': 1792080}\n",
            "{'loss': 0.9157, 'grad_norm': 6.154810905456543, 'learning_rate': 1.4861866979675154e-05, 'epoch': 0.63, 'num_input_tokens_seen': 1853968}\n",
            "{'loss': 0.8929, 'grad_norm': 11.35997486114502, 'learning_rate': 1.3370636335655656e-05, 'epoch': 0.65, 'num_input_tokens_seen': 1918224}\n",
            "{'loss': 0.9247, 'grad_norm': 7.251420974731445, 'learning_rate': 1.1930472654143776e-05, 'epoch': 0.67, 'num_input_tokens_seen': 1980864}\n",
            "{'loss': 0.9498, 'grad_norm': 12.12362289428711, 'learning_rate': 1.0547699994378787e-05, 'epoch': 0.69, 'num_input_tokens_seen': 2042080}\n",
            "{'loss': 0.8778, 'grad_norm': 6.937452793121338, 'learning_rate': 9.228390399647943e-06, 'epoch': 0.72, 'num_input_tokens_seen': 2103120}\n",
            "{'loss': 0.8978, 'grad_norm': 4.770256996154785, 'learning_rate': 7.978337233677268e-06, 'epoch': 0.74, 'num_input_tokens_seen': 2167184}\n",
            "{'loss': 0.8668, 'grad_norm': 3.7705488204956055, 'learning_rate': 6.803029740762648e-06, 'epoch': 0.76, 'num_input_tokens_seen': 2227680}\n",
            "{'loss': 0.9102, 'grad_norm': 6.93522310256958, 'learning_rate': 5.7076289413526554e-06, 'epoch': 0.78, 'num_input_tokens_seen': 2288528}\n",
            "{'loss': 0.8985, 'grad_norm': 3.9846746921539307, 'learning_rate': 4.696944968930847e-06, 'epoch': 0.8, 'num_input_tokens_seen': 2348144}\n",
            "{'loss': 0.8272, 'grad_norm': 6.484795093536377, 'learning_rate': 3.775415947715899e-06, 'epoch': 0.82, 'num_input_tokens_seen': 2409504}\n",
            "{'loss': 0.8489, 'grad_norm': 3.755187749862671, 'learning_rate': 2.9470885039321356e-06, 'epoch': 0.84, 'num_input_tokens_seen': 2472016}\n",
            " 84% 200/237 [1:44:43<18:41, 30.31s/it][INFO|trainer.py:3966] 2025-03-28 00:05:24,561 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200\n",
            "[INFO|configuration_utils.py:699] 2025-03-28 00:05:24,795 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-28 00:05:24,797 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:05:24,966 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:05:24,966 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:261] 2025-03-28 00:05:25,558 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:05:25,559 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:05:25,559 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/special_tokens_map.json\n",
            "[INFO|processing_utils.py:638] 2025-03-28 00:05:26,316 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-200/chat_template.json\n",
            "{'loss': 0.8049, 'grad_norm': 8.754579544067383, 'learning_rate': 2.2155999962293033e-06, 'epoch': 0.86, 'num_input_tokens_seen': 2533920}\n",
            "{'loss': 0.8734, 'grad_norm': 10.982963562011719, 'learning_rate': 1.5841625432818057e-06, 'epoch': 0.88, 'num_input_tokens_seen': 2594512}\n",
            "{'loss': 0.8221, 'grad_norm': 36.05628204345703, 'learning_rate': 1.0555489187053096e-06, 'epoch': 0.9, 'num_input_tokens_seen': 2654304}\n",
            "{'loss': 0.8983, 'grad_norm': 4.291540622711182, 'learning_rate': 6.320803752292464e-07, 'epoch': 0.93, 'num_input_tokens_seen': 2719440}\n",
            "{'loss': 0.876, 'grad_norm': 6.728209495544434, 'learning_rate': 3.1561645159166597e-07, 'epoch': 0.95, 'num_input_tokens_seen': 2782096}\n",
            "{'loss': 0.8385, 'grad_norm': 9.874861717224121, 'learning_rate': 1.0754680691665298e-07, 'epoch': 0.97, 'num_input_tokens_seen': 2844672}\n",
            "{'loss': 0.8843, 'grad_norm': 8.174927711486816, 'learning_rate': 8.785118431120509e-09, 'epoch': 0.99, 'num_input_tokens_seen': 2909088}\n",
            "100% 237/237 [2:04:26<00:00, 30.84s/it][INFO|trainer.py:3966] 2025-03-28 00:25:07,518 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237\n",
            "[INFO|configuration_utils.py:699] 2025-03-28 00:25:07,728 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-28 00:25:07,729 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:25:07,902 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:25:07,902 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:261] 2025-03-28 00:25:08,891 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:25:08,892 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:25:08,892 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/special_tokens_map.json\n",
            "[INFO|processing_utils.py:638] 2025-03-28 00:25:09,641 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/checkpoint-237/chat_template.json\n",
            "[INFO|trainer.py:2665] 2025-03-28 00:25:09,642 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 7468.6829, 'train_samples_per_second': 0.509, 'train_steps_per_second': 0.032, 'train_loss': 0.9357851527411224, 'epoch': 1.0, 'num_input_tokens_seen': 2933376}\n",
            "100% 237/237 [2:04:28<00:00, 31.51s/it]\n",
            "[INFO|image_processing_base.py:261] 2025-03-28 00:25:09,644 >> Image processor saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/preprocessor_config.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:25:09,644 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:25:09,645 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/special_tokens_map.json\n",
            "[INFO|processing_utils.py:638] 2025-03-28 00:25:10,386 >> chat template saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/chat_template.json\n",
            "[INFO|trainer.py:3966] 2025-03-28 00:25:10,386 >> Saving model checkpoint to saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42\n",
            "[INFO|configuration_utils.py:699] 2025-03-28 00:25:10,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-28 00:25:10,527 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-03-28 00:25:10,699 >> tokenizer config file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-03-28 00:25:10,699 >> Special tokens file saved in saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     0.9968\n",
            "  num_input_tokens_seen    =    2933376\n",
            "  total_flos               = 56780218GF\n",
            "  train_loss               =     0.9358\n",
            "  train_runtime            = 2:04:28.68\n",
            "  train_samples_per_second =      0.509\n",
            "  train_steps_per_second   =      0.032\n",
            "Figure saved at: saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42/training_loss.png\n",
            "[WARNING|2025-03-28 00:25:11] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-03-28 00:25:11] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2025-03-28 00:25:11,036 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkniz2V94fun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "4mu7m0dxbckJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_method bitsandbytes \\\n",
        "    --template qwen2_vl \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset crowd_eval \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100000 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir saves/Qwen2.5-VL-3B-Instruct/lora/qwen2.5-vl-3b-instruct-lora-visiontrainable-eval \\\n",
        "    --trust_remote_code True \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSYwv5nMbbKg",
        "outputId": "80802688-1afe-45c1-d011-05d5fd714b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-28 00:25:18.680114: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-03-28 00:25:18.698397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743121518.720289   55838 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743121518.726952   55838 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-28 00:25:18.749302: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-03-28 00:25:25] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:25,549 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-03-28 00:25:25,927 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|image_processing_base.py:381] 2025-03-28 00:25:26,081 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:381] 2025-03-28 00:25:26,120 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/preprocessor_config.json\n",
            "[WARNING|logging.py:329] 2025-03-28 00:25:26,120 >> Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO|image_processing_base.py:434] 2025-03-28 00:25:26,121 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-03-28 00:25:26,161 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-03-28 00:25:26,526 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|processing_utils.py:876] 2025-03-28 00:25:27,266 >> Processor Qwen2_5_VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 12845056,\n",
            "    \"shortest_edge\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
            "}\n",
            "\n",
            "[INFO|2025-03-28 00:25:27] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-03-28 00:25:27] llamafactory.data.loader:143 >> Loading dataset crowd_eval.jsonl...\n",
            "eval example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 198, 5501, 1760, 279, 1372, 315, 31912, 304, 279, 2168, 13, 5209, 3410, 697, 4226, 438, 264, 1372, 304, 279, 2701, 3561, 25, 1760, 151645, 198, 151644, 77091, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>\n",
            "Please count the number of sheep in the image. Please provide your answer as a number in the following format: count<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "label_ids:\n",
            "[22, 16, 151645, 198]\n",
            "labels:\n",
            "71<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-03-28 00:25:27,679 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-03-28 00:25:27,681 >> Model config Qwen2_5_VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2_5_VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"max_window_layers\": 70,\n",
            "  \"model_type\": \"qwen2_5_vl\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"depth\": 32,\n",
            "    \"fullatt_block_indexes\": [\n",
            "      7,\n",
            "      15,\n",
            "      23,\n",
            "      31\n",
            "    ],\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1280,\n",
            "    \"in_channels\": 3,\n",
            "    \"in_chans\": 3,\n",
            "    \"intermediate_size\": 3420,\n",
            "    \"model_type\": \"qwen2_5_vl\",\n",
            "    \"num_heads\": 16,\n",
            "    \"out_hidden_size\": 2048,\n",
            "    \"patch_size\": 14,\n",
            "    \"spatial_merge_size\": 2,\n",
            "    \"spatial_patch_size\": 14,\n",
            "    \"temporal_patch_size\": 2,\n",
            "    \"tokens_per_second\": 2,\n",
            "    \"window_size\": 112\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-03-28 00:25:27] llamafactory.model.patcher:143 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:1154] 2025-03-28 00:25:27,696 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2170] 2025-03-28 00:25:27,697 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-03-28 00:25:27,698 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2170] 2025-03-28 00:25:27,699 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.35s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-03-28 00:25:30,508 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-03-28 00:25:30,508 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-3B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-03-28 00:25:30,554 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/c747f21f03e7d0792c30766310bd7d8de17eeeb3/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-03-28 00:25:30,555 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 1e-06\n",
            "}\n",
            "\n",
            "[INFO|2025-03-28 00:25:30] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-03-28 00:25:32] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-03-28 00:25:32] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42\n",
            "[INFO|2025-03-28 00:25:32] llamafactory.model.loader:143 >> all params: 3,754,622,976\n",
            "[WARNING|2025-03-28 00:25:32] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n",
            "[INFO|trainer.py:4289] 2025-03-28 00:25:32,215 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4291] 2025-03-28 00:25:32,215 >>   Num examples = 600\n",
            "[INFO|trainer.py:4294] 2025-03-28 00:25:32,215 >>   Batch size = 2\n",
            "100% 300/300 [07:10<00:00,  1.44it/s]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.793 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "[INFO|integration_utils.py:831] 2025-03-28 00:32:45,727 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "100% 300/300 [2:43:37<00:00, 32.73s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4                 =     9.9718\n",
            "  predict_model_preparation_time =     0.0109\n",
            "  predict_rouge-1                =     8.3333\n",
            "  predict_rouge-2                =        0.0\n",
            "  predict_rouge-l                =     8.3333\n",
            "  predict_runtime                = 0:07:13.51\n",
            "  predict_samples_per_second     =      1.384\n",
            "  predict_steps_per_second       =      0.692\n",
            "[INFO|2025-03-28 03:09:11] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/Qwen2.5-VL-3B-Instruct/lora/qwen2.5-vl-3b-instruct-lora-visiontrainable-eval/generated_predictions.jsonl\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/LLaMA-Factory/wandb/offline-run-20250328_030911-czg5ccl6\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250328_030911-czg5ccl6/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lI3stNtUbbMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store results"
      ],
      "metadata": {
        "id": "i_A9RBjBbhjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "UTWizVg88v2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models to Google\n",
        "shutil.copytree('/content/LLaMA-Factory/saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-21-03-34',\n",
        "                '/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora')"
      ],
      "metadata": {
        "id": "a_d81LZmhqhv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0c3189f3-6a37-4392-e39c-a8852e4dc708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions\n",
        "shutil.copytree('/content/LLaMA-Factory/saves/Qwen2.5-VL-3B-Instruct/lora/qwen2.5-vl-3b-instruct-lora-eval',\n",
        "                '/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-eval')"
      ],
      "metadata": {
        "id": "VAdfFCQ-hq4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "6cf86e59-f6cd-4978-861f-55ede7ba3d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-eval'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copytree('/content/LLaMA-Factory/saves/Qwen2.5-VL-3B-Instruct/lora/train_2025-03-27-22-19-42',\n",
        "                '/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-visiontrainable')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "SNDxQqBQyl5-",
        "outputId": "20ae1b89-0617-400f-a7ec-65291db50fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-visiontrainable'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copytree('/content/LLaMA-Factory/saves/Qwen2.5-VL-3B-Instruct/lora/qwen2.5-vl-3b-instruct-lora-visiontrainable-eval',\n",
        "                '/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-visiontrainable-eval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "4dKrTpRyymEl",
        "outputId": "4d3523ef-f8b6-4c21-f533-2f091ec67dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CrowdLLM/qwen2.5-vl-3b-instruct-lora-visiontrainable-eval'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}